{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Nearest neighbor graphs](https://en.wikipedia.org/wiki/Nearest_neighbor_graph) are fundamental to work around outlier or anomaly detection, as well as implementing more sophisticated graph algorithms. Although Neo4J includes similarity metrics out of the box, constructing the nearest neighbors graph would require a brute force calculation, calculating pairwise similarity for *every* pair of nodes in the graph and then constructing a new graph based on those results. This is fine for small graphs, but prohibitively slow on real world data sets. \n",
    "\n",
    "To prototype some examples of how this can be done with a combination of Neo4J and Python, I'm looking at examples from four popular nearest neighbor libraries: sklearn's neighbors, and three approximate NN libraries (faster run times on large data) NMSLib, annoy, and pynndescent. Examples are below.\n",
    "\n",
    "## SKLearn / NearestNeighbors\n",
    "SKLearn supports NearestNeighbors implements unsupervised nearest neighbors learning. It acts as a uniform interface to three different nearest neighbors algorithms: BallTree, KDTree, and a brute-force algorithm based on routines in sklearn.metrics.pairwise - it does not allow specifying the distance function. In contrast to the subsequent libraries, this is an exact calculation so it may not scale well.\n",
    "\n",
    "### KD Tree: \n",
    "The KD tree is a binary tree structure which recursively partitions the parameter space along the data axes, dividing it into nested orthotropic regions into which data points are filed.\n",
    "\n",
    "### BallTree:\n",
    "Where KD trees partition data along Cartesian axes, ball trees partition data in a series of nesting hyper-spheres. This makes tree construction more costly than that of the KD tree, but results in a data structure which can be very efficient on highly structured data, even in very high dimensions.\n",
    "\n",
    "### example from the documentation\n",
    "**input:** numpy array\n",
    "\n",
    "**output:** list object with two arrays per input vector: one containing the IDs of the K-nearest neighbor nodes, and one containing the distances to those neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KDTree, BallTree\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_data = np.random.uniform(0, 1, size=(1000, 5))\n",
    "tree = KDTree(nn_data)\n",
    "tree_indices = tree.query(nn_data, 10, return_distance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "balltree = BallTree(nn_data)\n",
    "tree_indices = tree.query(nn_data, 10, return_distance=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nmslib\n",
    "Non-Metric Space Library (NMSLIB): An efficient similarity search library and a toolkit for evaluation of k-NN methods for generic non-metric spaces. This is a python wrapper for a C++ library. Methods in nmslib (specifically hnsw) are consistently the best performing algorithms in the [ANN benchmark dataset](http://ann-benchmarks.com/)\n",
    "\n",
    "### example from the documentation\n",
    "**input:** numpy array\n",
    "\n",
    "**output:** list object with two arrays per input vector: one containing the IDs of the K-nearest neighbor nodes, and one containing the distances to those neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmslib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random matrix to index\n",
    "data = numpy.random.randn(10000, 100).astype(numpy.float32)\n",
    "\n",
    "# initialize a new index, using a HNSW index on Cosine Similarity\n",
    "index = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "index.addDataPointBatch(data)\n",
    "index.createIndex({'post': 2}, print_progress=True)\n",
    "\n",
    "# query for the nearest neighbours of the first datapoint\n",
    "ids, distances = index.knnQuery(data[0], k=10)\n",
    "\n",
    "# get all nearest neighbours for all the datapoint using a pool of 4 threads to compute\n",
    "# outputs a list object with one row listing the ids of the n-nearest neighbors and the next row listing the distances\n",
    "neighbours = index.knnQueryBatch(data, k=10, num_threads=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyNNDescent\n",
    "\n",
    "PyNNDescent is a python library that implements the algorithm described in [Efficient K-Nearest Neighbor Graph Construction for Generic Similarity Measures](http://www.cs.princeton.edu/cass/papers/www11.pdf). It uses random projection trees for initialisation with a variety of metrics that are amenable to such approaches (euclidean, minkowski, angular, cosine, etc.). While it is not the top performing library in the [ANN Benchmark](http://ann-benchmarks.com/) it is written in python and could be easily forked to make use of Neo4J's internal similarity calculations, obviating the need to use an embedding and export from the graph.\n",
    "\n",
    "### Example from the documentation\n",
    "This codebase is a bit sparse and doesn't really have examples, so I'm guessing at the correct usage here... *fingers crossed* that this is reasonable.\n",
    "\n",
    "**input:** numpy array of vectors\n",
    "\n",
    "**output:** two numpy arrays, one with the indices (per row) and one with the distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynndescent import NNDescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_data = np.random.uniform(0, 1, size=(1000, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_indices, knn_dists = NNDescent(\n",
    "        nn_data, \"euclidean\", {}, 10, random_state=np.random\n",
    "    )._neighbor_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_indices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annoy\n",
    "Annoy (Approximate Nearest Neighbors Oh Yeah) is a C++ library with Python bindings to search for points in space that are close to a given query point. It also creates large read-only file-based data structures that are mmapped into memory so that many processes may share the same data. Annoy is produced by spotify, to power their recommendation engines. Annoy is almost as fast as the fastest libraries, but there is actually another feature that really sets Annoy apart: it has the ability to use static files as indexes. In particular, this means you can share index across processes. Annoy also decouples creating indexes from loading them, so you can pass around indexes as files and map them into memory quickly.\n",
    "\n",
    "### Example from the documentation\n",
    "**input**: vectors that describe each object which are used to populate the index\n",
    "\n",
    "**output:** ANN model, which can then be passed an index and return the k nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "import random\n",
    "\n",
    "# create an annoy index which has one item vector for each item to be used in the calculations. \n",
    "f = 40\n",
    "t = AnnoyIndex(f)  # Length of item vector that will be indexed\n",
    "for i in range(1000): # this builds an index with 1000 items which are each 40 digits long\n",
    "    v = [random.gauss(0, 1) for z in range(f)]\n",
    "    t.add_item(i, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next you build an ANN model - you can specify the number of trees - more trees = better model, but slower\n",
    "t.build(10) # 10 trees\n",
    "t.save('test.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 919, 139, 814, 632, 143, 941, 388, 413, 308]\n"
     ]
    }
   ],
   "source": [
    "# now to calculate neighbors, you load the model into an index and then use the get nns by item function.\n",
    "u = AnnoyIndex(f)\n",
    "u.load('test.ann') # super fast, will just mmap the file\n",
    "print(u.get_nns_by_item(1, 10)) # will find the 10 nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how can I use any of this with Neo4J?\n",
    "\n",
    "The simplest integration is to calculate some sort of node embedding, use those vectors for distance calculations, and then push the resulting data into Neo4J as a new graph. This is somewhat complicated (Neo4J doesn't really support it's embeddings, it's not technically possible to have to databases up at the same time) but let's try to make this work...\n",
    "\n",
    "A fancier solution might be to fork one of these libraries (PyNNDescent looks the easiest) and replace the internal distance calculations with calls to the in memory graph.\n",
    "\n",
    "Let's start by running the DeepGL embeddings on the GoT graph. Note that to use these embeddings, we'll need to download the jar file from https://github.com/neo4j-graph-analytics/ml-models/releases and adding the JAR file into the plugins folder of the database. Add the contents of package embeddings to the recognized procedures at the bottom of Manage->Settings. If you already have APOC and Graph Algorithms installed, it will look something like this: `dbms.security.procedures.whitelist=algo.*,apoc.*,embeddings.*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try running the DeepGL embedding on the GoT graph\n",
    "# note that to run this, you'll need to download the ml-models jar from"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
